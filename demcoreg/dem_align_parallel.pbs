#PBS -S /bin/bash
#PBS -V
### Note for higher res products, need more memory, use fewer cpus
### Note: parallel cannot efficiently spawn jobs to many nodes if they finish in <30 sec, scale accordingly
##PBS -lselect=64:model=ivy
##PBS -lselect=24:model=has
#PBS -lselect=64:model=bro
#PBS -lwalltime=2:00:00
#PBS -q devel 
#Email
#PBS -m abe
##Join stdout and stderr
##PBS -j oe

#To submit:
#qsub ~/src/demcoreg/demcoreg/dem_align_parallel.pbs

#To check progress
#ssh $(qstat -u deshean -W o=+rank0 | tail -n 1 | awk '{print $NF}')
#cd /PBS/spool

#Jobs can fail if stdout or stderr are >200MB
#Turn off automatic ls after cd
unset -f cd

#Set resource limits for max open files, no core dumps
ulimit -S -n 65536 -c 0

export GDAL_MAX_DATASET_POOL_SIZE=32768

#The ls -H here dereferences links
#Can shuffle sizes with `shuf` utility

#HMA
#refdem=/nobackup/deshean/refdem/hma/srtm1/hma_srtm_gl1.vrt
#refdem=/nobackup/deshean/data/nasadem/hma/hgt_srtmOnly_R4/srtmOnly.hgt/hma_nasadem_hgt_srtmOnly_R4_srtmOnly.hgt.vrt
#refdem=/nobackup/deshean/data/nasadem/hma/hgt_srtmOnly_R4/srtmOnly.hgt/hma_nasadem_hgt_srtmOnly_R4_srtmOnly.hgt_shift.vrt
#refdem=/nobackup/deshean/data/nasadem/hma/hgt_srtmOnly_R4/srtmOnly.hgt/hma_nasadem_hgt_srtmOnly_R4_srtmOnly.hgt_aea.tif
#refdem=/nobackup/deshean/data/nasadem/hma/hgt_srtmOnly_R4/srtmOnly.hgt/hma_nasadem_hgt_srtmOnly_R4_srtmOnly.hgt_lt5m_err_aea.tif
#refdem=/nobackup/deshean/data/nasadem/hma/hgt_merge/hgt/hma_nasadem_hgt_merge_hgt_adj_aea.tif
#refdem=/nobackup/deshean/data/nasadem/hma/hgt_merge/hgt/hma_nasadem_hgt_merge_hgt_adj.vrt
#refdem=/nobackup/deshean/data/tandemx/hma/TDM1_DEM_90m_hma_DEM.vrt
#refdem=/nobackup/deshean/data/tandemx/hma/TDM1_DEM_90m_hma_DEM_lt1.5m_err.vrt
#refdem=/nobackup/deshean/data/tandemx/hma/mos/TDM1_DEM_90m_hma_DEM_masked.vrt
#refdem=/nobackup/deshean/hma/dem_coreg/mos/hma_20181201_mos/hma_mos_8m/hma_mos_8m_wmean.vrt

refdem=/nobackupp8/deshean/hma/aster/dsm/mos/hma_20181206_mos/hma_mos_13/hma_mos_13_median.vrt

#WVGE
#topdir=/nobackup/deshean/hma/dem_coreg
#ASTER
topdir=/nobackup/deshean/hma/aster/dsm

#CONUS
#refdem=/nobackup/deshean/data/nasadem/conus/hgt_merge/hgt/conus_nasadem_hgt_merge_hgt_adj.vrt
#topdir=/nobackup/deshean/conus_combined/dem_coreg_nasadem

cd $topdir

###
### Filtering
###

#Filter
#max_dz=100
#max_dz=200

#done=$(ls -Sr *00/dem*/*dzfilt*tif)
#fn_list=$(ls -Sr *00/dem*/*DEM_*m.tif)
#notdone=""; for i in $fn_list ; do if ! echo $done | grep -q $(dirname $i) ; then notdone+=" $i" ; fi ; done
#parallel -j 10 --sshloginfile $PBS_NODEFILE "cd $topdir; ~/src/dgtools/dgtools/filter.py {} -filt dz -param $refdem -${max_dz} ${max_dz}" ::: $notdone

#fn_list=$(ls -Sr *00/dem*/*DEM_{32,8,2}m.tif)
#fn_list=$(ls -S *00/dem*/*DEM_2m.tif)
##parallel -j 10 --delay 0.1 --verbose --sshloginfile $PBS_NODEFILE "cd $topdir; if [ ! -e {.}_dzfilt_-${max_dz}_${max_dz}.tif ] ; then ~/src/pygeotools/pygeotools/filter.py {} -filt dz -param $refdem -${max_dz} ${max_dz} ; fi " ::: $fn_list
#parallel -j 6 --sshloginfile $PBS_NODEFILE "unset -f cd; cd $topdir; ~/src/pygeotools/pygeotools/filter.py {} -filt dz -param $refdem -${max_dz} ${max_dz}" ::: $fn_list

#fn_list=$(ls -S *00/dem*/*DEM_8m.tif)
#parallel -j 20 --sshloginfile $PBS_NODEFILE "unset -f cd; cd $topdir; ~/src/pygeotools/pygeotools/filter.py {} -filt dz -param $refdem -${max_dz} ${max_dz}" ::: $fn_list

#fn_list=$(ls -S *00/dem*/*DEM_32m.tif)
#parallel --sshloginfile $PBS_NODEFILE "unset -f cd; cd $topdir; ~/src/pygeotools/pygeotools/filter.py {} -filt dz -param $refdem -${max_dz} ${max_dz}" ::: $fn_list

###
### Co-registration
###

#log=log_dem_align
#if [ ! -d $log ] ; then
#    mkdir -pv $log
#fi

#ASTER
#fn_list=$(ls -H -S 2*/AST*DEM_cr.tif | shuf)
#ASTER uses limited memory, maybe 0.5 GB each
#parallel -j 56 --sshloginfile $PBS_NODEFILE "unset -f cd; cd $topdir; if [ ! -e {.}*align/*align.tif ] ; then ~/src/demcoreg/demcoreg/dem_align.py -res min $refdem {} > $log/{/.}.log 2>&1 ; fi" ::: $fn_list

#Do tiltcorr with crosstrack
##fn_list=$(ll -H -S *DEM_8m_dzfilt*.tif | grep 'alongtrack' | awk '{print $9}')

#WVGE
#Original files
#fn_list=$(ls -H -S *track/*DEM_8m_dzfilt*.tif | shuf)
#parallel -j 28 --sshloginfile $PBS_NODEFILE "unset -f cd; cd $topdir; if [ ! -e {.}*align/*align.tif ] ; then ~/src/demcoreg/demcoreg/dem_align.py -res common_scale_factor $refdem {} > $log/{/.}.log 2>&1; fi" ::: $fn_list
#parallel -j 28 --sshloginfile $PBS_NODEFILE "unset -f cd; cd $topdir; if [ ! -e {.}*align_tiltcorr/*align.tif ] ; then ~/src/demcoreg/demcoreg/dem_align.py $refdem -res common_scale_factor {} -tiltcorr > tee $log/{/.}.log 2>&1; fi" ::: $fn_list

#Apply filter mask to align.tif, can use as reference
#dem_align.py does this by default now
#fn_list=$(ls -H -S *track/2*align/*DEM_8m_dzfilt*align.tif | shuf)
#parallel -j 16 'apply_mask.py {} {.}_diff_filt.tif -extent raster' ::: fn_list

#Now run make_mos using align.tif or align_filt.tif

#Round 2
log=log_dem_align_round2
if [ ! -d $log ] ; then
    mkdir -pv $log
fi

#fn_list=$(ls -H -S alongtrack/2*align/*DEM_8m_dzfilt*align.tif | shuf)
#parallel -j 28 --sshloginfile $PBS_NODEFILE "unset -f cd; cd $topdir; if [ ! -e {.}*align/*align.tif ] ; then ~/src/demcoreg/demcoreg/dem_align.py -res min $refdem {} > $log/{/.}.log 2>&1; fi" ::: $fn_list
##parallel -j 28 --sshloginfile $PBS_NODEFILE "unset -f cd; cd $topdir; if [ ! -e {.}*align_tiltcorr/*align.tif ] ; then ~/src/demcoreg/demcoreg/dem_align.py $refdem -res common_scale_factor {} -tiltcorr > tee $log/{/.}.log 2>&1; fi" ::: $fn_list

fn_list=$(ls -H -S 2*/AST*DEM_cr_dem_align/*align.tif | shuf)
#ASTER uses limited memory, maybe 0.5 GB each
parallel -j 56 --sshloginfile $PBS_NODEFILE "unset -f cd; cd $topdir; if [ ! -e {.}*align/*align.tif ] ; then ~/src/demcoreg/demcoreg/dem_align.py -res min $refdem {} > $log/{/.}.log 2>&1 ; fi" ::: $fn_list

#For ASTER, now run abs_dz filter after co-registration
#fn_list=$(ls -S 2*/*align/*align.tif)
#parallel -j 28 --workdir $topdir --sshloginfile $PBS_NODEFILE "~/src/pygeotools/pygeotools/filter.py {} -filt dz -param $refdem -${max_dz} ${max_dz}" ::: $fn_list
